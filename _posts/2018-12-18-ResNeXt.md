---
layout:     post
title:      "Aggregated Residual Transformations for Deep Neural Networks"
subtitle:   "论文阅读笔记"
date:       2018-12-18 17:00:00
author:     "baiyf"
header-img: "img/about-bg-walle.jpg"
header-mask: 0.3
catalog:    true
tags:
    - Papers

---

# Aggregated Residual Transformations for Deep Neural Networks

## 写在前面

这是一篇发表在CVPR-2017上的论文，介绍了一种ResNet的升级版网络。相比于ResNet，在不增加参数数量的前提下，提高了性能表现：一个101层的ResNeXt网络和200层的ResNet准确度差不多，但是计算量只有后者的一半

## 主要思想

作者提出的ResNeXt，同时采用VGG堆叠的思想和Inception的**split-transform-merge**思想，但是为了可扩展，作者引入了一个**cardinality**的概念，代表一个block内分为多少个路径。作者认为这是一种不同于width和depth的全新的维度，同时比他们都更有效。下图是ResNet和本文ResNeXt的对比

![VGG_ResNext](\img\post\ResNet_ResNext.png)

**等价模式：**

作者也提出了几种ResNeXt block的等价结构,作者采用了(c)中的组卷积的方式，这种方法的优点是更加简洁并且训练速度更快

![ResNeXt_equ](\img\post\ResNeXt_equ.png)

## 网络结构

整个网络结构可以看作是多个block的堆叠，其中用cardinality（C）来控制网络的复杂度

![ResNeXt_arch](\img\post\ResNeXt_arch.png)

## 实验结论

- 网络达到了与ResNet在相同参数个数情况下，训练错误率更低的效果
- cardinality相比于其他超参数更有用
- 101层的ResNeXt比200层的ResNet效果更好

## 开源代码

作者开源了lua版本的Torch代码：[Implementation of a classification framework from the paper Aggregated Residual Transformations for Deep Neural Networks](https://github.com/facebookresearch/ResNeXt)

## 参考文献

1. [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf)
2. [ResNeXt——与 ResNet 相比，相同的参数个数，结果更好](https://www.cnblogs.com/bonelee/p/9031639.html)
3. [ResNeXt算法详解](https://blog.csdn.net/u014380165/article/details/71667916)